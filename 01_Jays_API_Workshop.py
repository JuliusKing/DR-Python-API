# -*- coding: utf-8 -*-
"""
Spyder Editor

This is a temporary script file.
"""

# create Cluster4_TS project
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import seaborn as sns

import pandas as pd
import datarobot as dr

from functools import reduce

from pandasql import sqldf

data_path = "/Users/julius.king/Sample Data/781670.f1/Readmission_Sample.csv"
Readmission = pd.read_csv(data_path, encoding ='latin')

Readmission.head()

# do a visualization on discharge disposition
print(Readmission.info())

discharge_counts = Readmission['discharge_description'].value_counts()
sns.set(style="darkgrid")
sns.barplot(discharge_counts.index, discharge_counts.values, alpha=0.9)
plt.title('Frequency Distribution of Discharge Descriptions')
plt.ylabel('Number of Occurrences', fontsize=12)
plt.xlabel('Discharge Discriptions', fontsize=12)
plt.show()

discharges = Readmission.groupby(['discharge_description', 'discharge_disposition_id'])
for p in discharges.groups:
    print(p, "has ", len(discharges.groups[p]), " entries")

# So we should exclude ID's 11, 19, 20, 21 - but in reality there were no 20 or 21 entries
Readmission_filtered = Readmission.query('discharge_disposition_id not in [11, 19]')
print(Readmission_filtered.info())

# Initialization with arguments my ID is FHmm1QZMffzF21g74owiOr7Z0l6bCSU7
dr.Client(token='', endpoint='https://app.datarobot.com/api/v2/')
# or include the yaml file
# dr.Client(config_path='~/Google Drive File Stream/My Drive/drconfig.yaml')

# Start the project
project = dr.Project.start(Readmission_filtered,
                           project_name='30-Day Diabetic Readmissions',
                           target="readmitted")
print('Project ID: {}'.format(project.id))

#  If running notebook remotely you can pop open a browser to view it
project.open_leaderboard_browser()

#  Set worker count higher.
#  Passing -1 sets it to the maximum available to your account.
project.set_worker_count(-1)

project.pause_autopilot()

#  More jobs will go in the queue in each stage of autopilot.
#  This gets the currently inprogress and queued jobs
project.get_model_jobs()

project.unpause_autopilot()

# We can get the features
features = project.get_features()
features

pd.DataFrame([f.__dict__ for f in features])

# Look at other autogenerated feature lists
feature_lists = project.get_featurelists()
feature_lists

# Create a feature list without race
# (since it could be sensitive)
informative_feats = [lst for lst in feature_lists if
                     lst.name == 'Informative Features'][0]
no_race_features = list(
    set(informative_feats.features) - {'race'})

no_race = project.create_featurelist('no race',
                                              no_race_features)
no_race

project.get_status()

# This waits until autopilot is complete:
project.wait_for_autopilot(check_interval=90)

# now start with the new feature list
project.start_autopilot(no_race.id)

project.wait_for_autopilot(check_interval=90)

# Let's look at some models now!
models = project.get_models()
example_model = models[0]
example_model

example_model.metrics

def sorted_by_log_loss(models, test_set):
    models_with_score = [model for model in models if
                         model.metrics['LogLoss'][test_set] is not None]
    return sorted(models_with_score,
                  key=lambda model: model.metrics['LogLoss'][test_set])
    
# Look at best models with and without race
    models = project.get_models()
models_wo_race = [mod for mod in models if
               mod.featurelist_id == no_race.id]
models_w_race = [mod for mod in models if
                     mod.featurelist_id == informative_feats.id]

best_wo_race_model = sorted_by_log_loss(models_wo_race, 'crossValidation')[0]
best_w_race_model = sorted_by_log_loss(models_w_race, 'crossValidation')[0]
best_wo_race_model.metrics, best_w_race_model.metrics

# Top models are Random Forrest Classifiers, we'll use the one with race
best_w_race_model = sorted_by_log_loss(models_w_race, 'crossValidation')[0]
best_w_race_model.model_type

# Model visualizations
feature_impacts = best_w_race_model.get_or_request_feature_impact()

dr_dark_blue = '#08233F'
dr_blue = '#1F77B4'
dr_orange = '#FF7F0E'
dr_red = '#BE3C28'

# Formats the ticks from a float into a percent
percent_tick_fmt = mtick.PercentFormatter(xmax=1.0)

impact_df = pd.DataFrame(feature_impacts)
impact_df.sort_values(by='impactNormalized', ascending=True, inplace=True)

# Positive values are blue, negative are red
bar_colors = impact_df.impactNormalized.apply(lambda x: dr_red if x < 0
                                              else dr_blue)

ax = impact_df.plot.barh(x='featureName', y='impactNormalized',
                         legend=False,
                         color=bar_colors,
                         figsize=(10, 8))
ax.xaxis.set_major_formatter(percent_tick_fmt)
ax.xaxis.set_tick_params(labeltop=True)
ax.xaxis.grid(True, alpha=0.2)
ax.set_facecolor(dr_dark_blue)

plt.ylabel('')
plt.xlabel('Effect')
plt.xlim((None, 1))  # Allow for negative impact
plt.title('Feature Impact', y=1.04)

# Let's look at average impact of several models
def get_impact_as_df(model):
    impact_df = pd.DataFrame(model.get_or_request_feature_impact())
    impact_df['model_name'] = model.model_type
    return(impact_df)

n = 5
top_n_models = sorted_by_log_loss(models_wo_race, 'crossValidation')[4:n+4]
top_impacts = [get_impact_as_df(model) for model in top_n_models]
impact_df = pd.concat(top_impacts)

impact_df = impact_df.groupby('featureName', as_index=False)['impactNormalized'].mean()

# Formats the ticks from a float into a percent
percent_tick_fmt = mtick.PercentFormatter(xmax=1.0)

impact_df.sort_values(by='impactNormalized', ascending=True, inplace=True)

# Positive values are blue, negative are red
bar_colors = impact_df.impactNormalized.apply(lambda x: dr_red if x < 0
                                              else dr_blue)

ax = impact_df.plot.barh(x='featureName', y='impactNormalized',
                         legend=False,
                         color=bar_colors,
                         figsize=(10, 8))
ax.xaxis.set_major_formatter(percent_tick_fmt)
ax.xaxis.set_tick_params(labeltop=True)
ax.xaxis.grid(True, alpha=0.2)
ax.set_facecolor(dr_dark_blue)

plt.ylabel('')
plt.xlabel('Effect')
plt.xlim((None, 1))  # Allow for negative impact
plt.title('Feature Impact', y=1.04)

# Unlock the project holdout
project.unlock_holdout()

best_wo_race_model = dr.Model.get(project.id, best_wo_race_model.id)
best_w_race_model = dr.Model.get(project.id, best_w_race_model.id)

best_w_race_model.metrics['LogLoss'], best_w_race_model.metrics['LogLoss']

# Retrain on 100%
model_job_w_race_100pct_id = best_w_race_model.train(sample_pct=100)
model_job_w_race_100pct_id

model_w_race_100pct = dr.models.modeljob.wait_for_async_model_creation(
    project.id, model_job_w_race_100pct_id)
model_w_race_100pct.id

# Let's make some predictions
scoring_df = pd.read_csv("/Users/julius.king/Sample Data/781670.f1/1Scoring_Sample.csv", encoding ='latin')
scoring_modeling = scoring_df.query('discharge_disposition_id not in [11, 19]')
scoring_modeling.head()

prediction_dataset = project.upload_dataset(scoring_modeling)
predict_job = model_w_race_100pct.request_predictions(prediction_dataset.id)
prediction_dataset.id

predictions = predict_job.get_result_when_complete()

pd.concat([scoring_modeling, predictions], axis=1).head()

# Let's check the results
%matplotlib inline
import seaborn as sns
import matplotlib.pyplot as plt
import matplotlib

matplotlib.rcParams['figure.figsize'] = (15, 10)  # make charts bigger

sns.set(color_codes=True)
sns.kdeplot(predictions.positive_probability, shade=True, cut=0,
            label='Positive Probability')
plt.xlim((0, 1))
plt.ylim((0, None))
plt.xlabel('Probability of Event')
plt.ylabel('Probability Density')
plt.title('Prediction Distribution')

# Cluster on prediction explanations
# #####################################################################
# Generic method to do everything required to retrieve the scores and
# explanations for a given data set.
# #####################################################################
def retrieve_prediction_explanations(proj, mod, pdata, n_reasons = 5):
    # UPLOAD THE DATASET
    dataset = proj.upload_dataset(pdata) # Returns an instance of [PredictionDataset]
    pred_job = mod.request_predictions(dataset.id)
    preds = pred_job.get_result_when_complete()
    # NOW WE NEED TO ENSURE THAT FEATURE IMPACT EXISTS FOR THAT MODEL
    try:
        impact_job = mod.request_feature_impact()
        impact_job.wait_for_completion(300)
    except dr.errors.JobAlreadyRequested:
        pass  # already computed
    # NOW ENSURE THAT THE PREDICTION EXPLANATIONS ARE COMPUTED 
    try:
        dr.PredictionExplanationsInitialization.get(proj.id, mod.id)
    except dr.errors.ClientError as e:
        assert e.status_code == 404  # haven't been computed
        init_job = dr.PredictionExplanationsInitialization.create(proj.id, mod.id)
        init_job.wait_for_completion()
    # RUN THE REASON CODE JOB
    rc_job = dr.PredictionExplanations.create(proj.id,
                               mod.id,
                               dataset.id,
                               max_explanations=n_reasons,
                               threshold_low=None,
                               threshold_high=None)
    rc = rc_job.get_result_when_complete(max_wait=1200)
    all_rows = rc.get_all_as_dataframe()
    return all_rows

exp = retrieve_prediction_explanations(project, best_wo_race_model, 
                                       Readmission_filtered,
                                       n_reasons = 6)

# ######################################################################
# UTILITY FUNCTIONS FOR CLEAN RE_USABLE BEHAVIOUR - thanks to John Hawkins for this piece
# ######################################################################
def unlist(listOfLists):
    return [item for sublist in listOfLists for item in sublist]

def unique_elements(bigList):
    return reduce(lambda l, x: l.append(x) or l if x not in l else l, bigList, [])

# #############################################################
# TRANSFORMATION OF THE PREDICTION EXPLANATIONS INTO A SET OF 
# COLUMNS PER FEATURE WITH THE QUANTITATIVE PREDICTION STRENGTH 
# VALUE IN THE DATA CELL
# WE NEED TO KNOW THE PROJECT TYPE TO DETERMINE THE COLUMN NUMBER
# WHERE THE EXPLANATIONS START.
# #############################################################
def get_strength_per_feature_cols(proj, all_rows, n_reasons=5):
    colsToUse = []
    startPoint = 6
    if proj.target_type == 'Regression':
        startPoint = 2
    if proj.target_type == 'Binary':
        startPoint = 6
    j = startPoint
    for i in range(n_reasons):
        colsToUse.append(j) 
        colsToUse.append(j+4)
        j = j + 5 
    rc3 = all_rows.iloc[:, colsToUse]
    j = 0
    colsForNames = []
    for i in range(n_reasons):
        colsForNames.append(j) 
        j = j + 2 
    namesdf = rc3.iloc[:,colsForNames]
    allfeatures = [namesdf[i].unique().tolist() for i in namesdf.columns]
    nameslist = unique_elements(unlist(allfeatures))
    ####################################################################
    # CREATE A NEW DATAFRAME WITH ONE COLUMN PER POSSIBLE REASON CODE
    # INITIALISE TO ZERO AND THEN FILL WITH THE EXPLANATION STRENGTHS
    ####################################################################
    dfnew = pd.DataFrame(columns=nameslist)
    for j in range(len(rc3)):
        dfnew.loc[j] = [0 for n in range(len(nameslist))]
        for i in range(n_reasons):
            rcname = rc3.loc[j][i*2] 
            rcvalue = rc3.loc[j][i*2+1]
            dfnew.loc[j][rcname] = rcvalue    
    return dfnew

flattened = get_strength_per_feature_cols(project, exp, n_reasons=6)

flattened

import umap
import hdbscan
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
%matplotlib inline
sns.set(style='white', rc={'figure.figsize':(10,8)})

#Recommended settings for UMAP, but can use other dimensionality reduction techniques.  N_neighbors influences the density 
#requirement per cluster
clusterable_embedding = umap.UMAP(
    n_neighbors=30,
    min_dist=0.0,
    n_components=2,
    random_state=42,
).fit_transform(flattened)

print(flattened.shape)
print(clusterable_embedding.shape)

plt.scatter(clusterable_embedding[:, 0], clusterable_embedding[:, 1], s=0.1);

###Get cluster labels for your dataset
labels = hdbscan.HDBSCAN(
    min_samples=10,
    min_cluster_size=500,
).fit_predict(clusterable_embedding)

##See how many clusters 
set(labels)

#NB - -1 is a cluster of "noise" for HDBSCAN and should not be viewed as an actual cluster

clustered = (labels >= 0)  ###  the -1 cluster is noise within HDBSCAN
plt.scatter(clusterable_embedding[~clustered, 0],
            clusterable_embedding[~clustered, 1],
            c=(0.5, 0.5, 0.5),
            s=0.1,
            alpha=0.5)
plt.scatter(clusterable_embedding[clustered, 0],
            clusterable_embedding[clustered, 1],
            c=labels[clustered],
            s=0.1,
            cmap='winter');
            
# Advanced: Model Factory byt admission type
admission_ids = ['Elective', 'Urgent', 'Other', 'Emergency']
Elective = Readmission_filtered.query('admit_type_description == "Elective"')
Urgent = Readmission_filtered.query('admit_type_description == "Urgent"')
Emergency = Readmission_filtered.query('admit_type_description == "Emergency"')
Other = Readmission_filtered.query('admit_type_description not in ["Elective", "Urgent", "Emergency"]')
dataset_list = [ Elective, Urgent, Other, Emergency ]

# Run projects
projects = list()
for i in range(4):
    projects.append(dr.Project.start(dataset_list[i],
                                     project_name=project_names[i],
                                     target="readmitted",
                                     worker_count=-1,
                                     autopilot_on=True))
